# -*- coding: utf-8 -*-
"""cancer-lgn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d4L8NKG0RO70mG10AV4Dh1zp1q52oOYT
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import random
import matplotlib.pyplot as plt
from scipy.stats import norm
import json
import csv
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler

# Seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# Load and Binarize Cancer Dataset
def generate_dataset(n_samples=569, n_features=8):
    # Load Wisconsin Breast Cancer Dataset
    data = load_breast_cancer()
    df = pd.DataFrame(data.data, columns=data.feature_names)

# Add the 'target' (the label: 0 or 1) as a new column
    df['target'] = data.target
    X = data.data  # Shape: (569, 30)
    y = data.target  # Shape: (569,), 0=benign (Class 0), 1=malignant (Class 1)

    # Select top 8 features (based on importance, e.g., radius_mean, perimeter_worst)
    feature_indices = [0, 2, 3, 6, 20, 22, 23, 26]  # radius_mean, perimeter_mean, area_mean, compactness_mean, radius_worst, perimeter_worst, area_worst, compactness_worst
    X_selected = X[:, feature_indices]  # Shape: (569, 8)

    # Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_selected)

    # Binarize features (threshold at median)
    X_binary = np.zeros_like(X_scaled, dtype=int)
    for i in range(X_scaled.shape[1]):
        median = np.median(X_scaled[:, i])
        X_binary[:, i] = (X_scaled[:, i] > median).astype(int)

    # Labels are already 0 (benign) and 1 (malignant), matching Class 0 and Class 1
    indices = np.random.permutation(len(X_binary))
    X_binary, y = X_binary[indices], y[indices]

    # Return raw features for EDA (use first feature for visualization)
    return X_selected[:, 0], y, X_binary

#  Preprocess and Binarize Data
def binarize_data(data, n_bits=8):
    data_min, data_max = data.min(), data.max()
    normalized = (data - data_min) / (data_max - data_min)
    binary_data = np.zeros((len(data), n_bits))
    for i in range(len(data)):
        value = int(normalized[i] * (2**n_bits - 1))
        binary = [int(b) for b in bin(value)[2:].zfill(n_bits)]
        binary_data[i] = binary
    return binary_data

#  Define Logic Gates
logic_gates = {
    0: lambda a, b: torch.zeros_like(a),                    # False
    1: lambda a, b: a * b,                                 # AND
    2: lambda a, b: a * (1 - b),                           # A ∧ ¬B
    3: lambda a, b: a,                                     # A
    4: lambda a, b: (1 - a) * b,                           # ¬A ∧ B
    5: lambda a, b: b,                                     # B
    6: lambda a, b: a + b - 2 * a * b,                     # XOR
    7: lambda a, b: a + b - a * b,                         # OR
    8: lambda a, b: 1 - (a + b - a * b),                   # NOR
    9: lambda a, b: 1 - (a + b - 2 * a * b),               # XNOR
    10: lambda a, b: 1 - b,                                # ¬B
    11: lambda a, b: 1 - (1 - a) * b,                      # A ∨ ¬B
    12: lambda a, b: 1 - a,                                # ¬A
    13: lambda a, b: 1 - a * (1 - b),                      # A ∨ B
    14: lambda a, b: 1 - a * b,                            # NAND
    15: lambda a, b: torch.ones_like(a)                    # True
}

gate_names = {
    0: "FALSE", 1: "AND", 2: "A_AND_NOT_B", 3: "A", 4: "NOT_A_AND_B", 5: "B",
    6: "XOR", 7: "OR", 8: "NOR", 9: "XNOR", 10: "NOT_B", 11: "A_OR_NOT_B",
    12: "NOT_A", 13: "A_OR_B", 14: "NAND", 15: "TRUE"
}

#  Define Differentiable Logic Gate Network (unchanged)
class LogicGateNetwork(nn.Module):
    def __init__(self, n_inputs, n_layers, n_neurons_per_layer):
        super(LogicGateNetwork, self).__init__()
        self.n_inputs = n_inputs
        self.n_layers = n_layers
        self.n_neurons_per_layer = n_neurons_per_layer
        self.n_gates = 16

        self.connections = []
        for layer in range(n_layers):
            layer_connections = []
            input_size = n_inputs if layer == 0 else n_neurons_per_layer
            for _ in range(n_neurons_per_layer):
                input1, input2 = random.sample(range(input_size), 2)
                layer_connections.append((input1, input2))
            self.connections.append(layer_connections)

        self.gate_weights = nn.ParameterList([
            nn.Parameter(torch.randn(n_neurons_per_layer, self.n_gates))
            for _ in range(n_layers)
        ])

        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        current = x
        for layer in range(self.n_layers):
            next_layer = []
            gate_probs = self.softmax(self.gate_weights[layer])
            for neuron in range(self.n_neurons_per_layer):
                input1_idx, input2_idx = self.connections[layer][neuron]
                a1, a2 = current[:, input1_idx], current[:, input2_idx]
                output = torch.zeros_like(a1)
                for gate_id in range(self.n_gates):
                    gate_output = logic_gates[gate_id](a1, a2)
                    output += gate_probs[neuron, gate_id] * gate_output
                next_layer.append(output.unsqueeze(1))
            current = torch.cat(next_layer, dim=1)
        return current

#  Aggregate Outputs for Binary Classification (unchanged)
def aggregate_outputs(outputs, n_outputs_per_class, tau=1.0):
    n_classes = 2
    class_scores = []
    for i in range(n_classes):
        start = i * n_outputs_per_class
        end = (i + 1) * n_outputs_per_class
        score = outputs[:, start:end].sum(dim=1) / tau
        class_scores.append(score.unsqueeze(1))
    return torch.cat(class_scores, dim=1)

#  Discretize Network for Logisim (unchanged)
def discretize_network(model):
    discretized = []
    for layer in range(model.n_layers):
        layer_gates = []
        gate_probs = model.softmax(model.gate_weights[layer]).detach().numpy()
        for neuron in range(model.n_neurons_per_layer):
            gate_id = np.argmax(gate_probs[neuron])
            input1, input2 = model.connections[layer][neuron]
            layer_gates.append((gate_id, input1, input2))
        discretized.append(layer_gates)
    return discretized

#  Generate Logisim Instructions (unchanged)
def generate_logisim_instructions(discretized, n_inputs, n_layers, n_neurons_per_layer, n_outputs_per_class):
    instructions = []
    instructions.append("Logisim Logic Gate Network Blueprint")
    instructions.append(f"Number of Input Bits: {n_inputs}")
    instructions.append(f"Number of Layers: {n_layers}")
    instructions.append(f"Neurons per Layer: {n_neurons_per_layer}")
    instructions.append(f"Outputs per Class: {n_outputs_per_class}")
    instructions.append("\nLayer Connections and Gates:")

    for layer in range(n_layers):
        instructions.append(f"\nLayer {layer + 1}:")
        input_source = "Input Bits" if layer == 0 else f"Layer {layer}"
        for neuron, (gate_id, input1, input2) in enumerate(discretized[layer]):
            gate_name = gate_names[gate_id]
            instructions.append(
                f"  Neuron {neuron + 1}: {gate_name} gate, "
                f"Inputs from {input_source} [Index {input1 + 1}, Index {input2 + 1}]"
            )

    instructions.append("\nOutput Aggregation:")
    instructions.append("  Binary Classification (2 classes):")
    for class_id in range(2):
        start = class_id * n_outputs_per_class + 1
        end = (class_id + 1) * n_outputs_per_class
        neurons = list(range(start, end + 1))
        instructions.append(
            f"  Class {class_id}: Sum outputs of Layer {n_layers} Neurons {neurons} "
            f"using a binary adder (implemented with AND, XOR, OR gates)"
        )
    instructions.append(
        "  Final Output: Compare sums (Class 0 vs Class 1) using a comparator "
        "to select class with higher sum"
    )

    return "\n".join(instructions)

#  Enhanced EDA Visualizations
def perform_eda(data, labels):
    #  Histogram with Normal Distribution
    plt.figure(figsize=(10, 6))
    plt.hist(data, bins=50, density=True, alpha=0.7, color='blue', label='Radius Mean Distribution')
    mu, sigma = np.mean(data), np.std(data)
    x = np.linspace(min(data), max(data), 100)
    #plt.plot(x, norm.pdf(x, mu, sigma), 'r-', lw=2, label='Normal Distribution')
    plt.title('Radius Mean Distribution')
    plt.xlabel('Radius Mean')
    plt.ylabel('Density')
    plt.legend()
    plt.savefig('dataset_distribution.png')
    plt.close()

    #  Scatter Plot by Class
    plt.figure(figsize=(10, 6))
    plt.scatter(data[labels == 0], np.zeros_like(data[labels == 0]), c='blue', label='Benign (Class 0)', alpha=0.5)
    plt.scatter(data[labels == 1], np.ones_like(data[labels == 1]), c='red', label='Malignant (Class 1)', alpha=0.5)
    plt.title('Radius Mean by Class')
    plt.xlabel('Radius Mean')
    plt.ylabel('Class')
    plt.yticks([0, 1], ['Benign', 'Malignant'])
    plt.legend()
    plt.savefig('data_scatter_by_class.png')
    plt.close()

    #  Box Plot
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=data, orient='h', color='green')
    plt.title('Box Plot of Radius Mean')
    plt.xlabel('Radius Mean')
    plt.savefig('dataset_boxplot.png')
    plt.close()

    #  Class Distribution Bar Plot
    plt.figure(figsize=(10, 6))
    unique, counts = np.unique(labels, return_counts=True)
    plt.bar(unique, counts, color=['blue', 'red'], tick_label=['Benign', 'Malignant'])
    plt.title('Class Distribution')
    plt.xlabel('Class')
    plt.ylabel('Count')
    plt.savefig('class_distribution.png')
    plt.close()

#  Plot Training Metrics (unchanged)
def plot_training_metrics(train_accs, test_accs, train_losses, epochs):
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_accs, 'b-', label='Training Accuracy')
    plt.plot(epochs, test_accs, 'r-', label='Testing Accuracy')
    plt.title('Training and Testing Accuracies Over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.savefig('accuracies_over_epochs.png')
    plt.close()

    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_losses, 'g-', label='Training Loss')
    plt.title('Training Loss Over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.savefig('loss_over_epochs.png')
    plt.close()

# Save Network Configuration (unchanged)
def save_network_config(discretized, n_inputs, n_layers, n_neurons_per_layer, n_outputs_per_class):
    config = {
        "n_inputs": n_inputs,
        "n_layers": n_layers,
        "n_neurons_per_layer": n_neurons_per_layer,
        "n_outputs_per_class": n_outputs_per_class,
        "discretized_network": [
            [
                {
                    "neuron": neuron + 1,
                    "gate_id": int(gate_id),
                    "gate_name": gate_names[gate_id],
                    "input1": int(input1) + 1,
                    "input2": int(input2) + 1
                }
                for neuron, (gate_id, input1, input2) in enumerate(layer)
            ]
            for layer in discretized
        ]
    }
    with open("network_config.json", "w") as f:
        json.dump(config, f, indent=4)

#  Save Dataset (unchanged)
def save_dataset(binary_data, labels):
    with open("dataset.csv", "w", newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["Input1", "Input2", "Input3", "Input4", "Input5", "Input6", "Input7", "Input8", "Label"])
        for binary, label in zip(binary_data, labels):
            writer.writerow(list(binary) + [int(label)])

#  Main Execution (adapted for cancer dataset)
def main():
    # Generate dataset and perform EDA
    data, labels, binary_data = generate_dataset()
    perform_eda(data, labels)

    # Use pre-binarized data
    X = torch.tensor(binary_data, dtype=torch.float32)
    y = torch.tensor(labels, dtype=torch.long)

    # Split data (369 train, 200 test to match dataset size)
    n_samples = len(X)
    n_train = 369
    X_train, X_test = X[:n_train], X[n_train:]
    y_train, y_test = y[:n_train], y[n_train:]

    # Save dataset
    save_dataset(binary_data, labels)

    # Initialize model
    n_inputs = 8
    n_layers = 4
    n_neurons_per_layer = 8
    n_outputs_per_class = 4
    model = LogicGateNetwork(n_inputs, n_layers, n_neurons_per_layer)

    # Training setup
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    n_epochs = 100
    train_accs = []
    test_accs = []
    train_losses = []
    epochs = []

    # Training loop with accuracy and loss tracking
    for epoch in range(n_epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X_train)
        scores = aggregate_outputs(outputs, n_outputs_per_class, tau=0.1)
        loss = criterion(scores, y_train)
        loss.backward()
        optimizer.step()

        # Calculate training accuracy and loss
        with torch.no_grad():
            _, predicted = torch.max(scores, 1)
            train_accuracy = (predicted == y_train).float().mean().item()
            train_accs.append(train_accuracy)
            train_losses.append(loss.item())

        # Calculate testing accuracy
        model.eval()
        with torch.no_grad():
            outputs = model(X_test)
            scores = aggregate_outputs(outputs, n_outputs_per_class, tau=0.1)
            _, predicted = torch.max(scores, 1)
            test_accuracy = (predicted == y_test).float().mean().item()
            test_accs.append(test_accuracy)

        epochs.append(epoch + 1)

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch + 1}, Loss: {loss.item():.4f}, "
                  f"Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}")

    # Final test accuracy
    model.eval()
    with torch.no_grad():
        outputs = model(X_test)
        scores = aggregate_outputs(outputs, n_outputs_per_class, tau=0.1)
        _, predicted = torch.max(scores, 1)
        final_accuracy = (predicted == y_test).float().mean().item()
        print(f"Final Test Accuracy: {final_accuracy:.4f}")

    # Plot training metrics
    plot_training_metrics(train_accs, test_accs, train_losses, epochs)

    # Discretize and save network
    discretized = discretize_network(model)
    save_network_config(discretized, n_inputs, n_layers, n_neurons_per_layer, n_outputs_per_class)

    # Generate Logisim instructions
    instructions = generate_logisim_instructions(
        discretized, n_inputs, n_layers, n_neurons_per_layer, n_outputs_per_class
    )
    print("\nLogisim Instructions:\n")
    print(instructions)

    with open("logisim_instructions.txt", "w") as f:
        f.write(instructions)

if __name__ == "__main__":
    main()